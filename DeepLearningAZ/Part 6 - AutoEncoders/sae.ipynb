{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHwJAy8eaK0rxUHsVrbtUp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"r_T6DwmhLeEU"},"source":["# AutoEncoders"]},{"cell_type":"markdown","metadata":{"id":"mRfllOFCeNIe"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"ZVdzTkqHehem"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncWa_a4Zek9k"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"bvcjT5Uqeqrj"},"source":["movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VxJyeJVSeuYF"},"source":["## Preparing the training set and the test set"]},{"cell_type":"code","metadata":{"id":"bXy3oK_5e26x"},"source":["training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n","training_set = np.array(training_set, dtype = 'int')\n","test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n","test_set = np.array(test_set, dtype = 'int')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6AZsqV0fBth"},"source":["## Getting the number of users and movies"]},{"cell_type":"code","metadata":{"id":"pjYBgQJ4fHXp"},"source":["nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n","nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BduAm9XTfJ5M"},"source":["## Converting the data into an array with users in lines and movies in columns"]},{"cell_type":"code","metadata":{"id":"2hvNwdPwfNJa"},"source":["def convert(data):\n","    new_data = []\n","    for id_users in range(1, nb_users + 1):\n","        id_movies = data[:,1][data[:,0] == id_users]\n","        id_ratings = data[:,2][data[:,0] == id_users]\n","        ratings = np.zeros(nb_movies)\n","        ratings[id_movies - 1] = id_ratings\n","        new_data.append(list(ratings))\n","    return new_data\n","training_set = convert(training_set)\n","test_set = convert(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDMTdRohfca9"},"source":["## Converting the data into Torch tensors"]},{"cell_type":"code","metadata":{"id":"dCrtcCCqfgNc"},"source":["training_set = torch.FloatTensor(training_set)\n","test_set = torch.FloatTensor(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68DIciQ1fifc"},"source":["## Creating the architecture of the Neural Network"]},{"cell_type":"code","metadata":{"id":"U3JKRMIXfmTr"},"source":["class SAE(nn.Module):\n","    def __init__(self, ):\n","        super(SAE, self).__init__()\n","        self.fc1 = nn.Linear(nb_movies, 20)\n","        self.fc2 = nn.Linear(20, 10)\n","        self.fc3 = nn.Linear(10, 20)\n","        self.fc4 = nn.Linear(20, nb_movies)\n","        self.activation = nn.Sigmoid()\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x))\n","        x = self.activation(self.fc2(x))\n","        x = self.activation(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","sae = SAE()\n","criterion = nn.MSELoss()\n","optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zn8Zl34Ofpe_"},"source":["## Training the SAE"]},{"cell_type":"code","metadata":{"id":"89ACdnopfwrG","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"error","timestamp":1727886369476,"user_tz":-240,"elapsed":427,"user":{"displayName":"Hadelin de Ponteves","userId":"15047218817161520419"}},"outputId":"a1843411-22aa-4816-ca7b-513929b2f28e"},"source":["nb_epoch = 200\n","for epoch in range(1, nb_epoch + 1):\n","    train_loss = 0\n","    s = 0.\n","    for id_user in range(nb_users):\n","        input = Variable(training_set[id_user]).unsqueeze(0)\n","        target = input.clone()\n","        if torch.sum(target.data > 0) > 0:\n","            output = sae(input)\n","            target.require_grad = False\n","            output[target == 0] = 0\n","            loss = criterion(output, target)\n","            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n","            loss.backward()\n","            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n","            s += 1.\n","            optimizer.step()\n","    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4eab0d08adfa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmean_corrector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_movies\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmean_corrector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"]}]},{"cell_type":"code","source":["nb_epoch = 200\n","for epoch in range(1, nb_epoch + 1):\n","    train_loss = 0\n","    s = 0.\n","    for id_user in range(nb_users):\n","        input = Variable(training_set[id_user]).unsqueeze(0)\n","        target = input.clone()\n","        if torch.sum(target.data > 0) > 0:\n","            output = sae(input)\n","            target.require_grad = False\n","            output[target == 0] = 0\n","            loss = criterion(output, target)\n","            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n","            loss.backward()\n","            train_loss += np.sqrt(loss.data*mean_corrector)\n","            s += 1.\n","            optimizer.step()\n","    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBk2APK-FLow","executionInfo":{"status":"ok","timestamp":1727887370376,"user_tz":-240,"elapsed":393492,"user":{"displayName":"Hadelin de Ponteves","userId":"15047218817161520419"}},"outputId":"f3019b3c-555a-4043-8d53-43c12daf22b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1 loss: tensor(1.7560)\n","epoch: 2 loss: tensor(1.0963)\n","epoch: 3 loss: tensor(1.0533)\n","epoch: 4 loss: tensor(1.0383)\n","epoch: 5 loss: tensor(1.0309)\n","epoch: 6 loss: tensor(1.0265)\n","epoch: 7 loss: tensor(1.0239)\n","epoch: 8 loss: tensor(1.0218)\n","epoch: 9 loss: tensor(1.0208)\n","epoch: 10 loss: tensor(1.0196)\n","epoch: 11 loss: tensor(1.0189)\n","epoch: 12 loss: tensor(1.0184)\n","epoch: 13 loss: tensor(1.0179)\n","epoch: 14 loss: tensor(1.0174)\n","epoch: 15 loss: tensor(1.0173)\n","epoch: 16 loss: tensor(1.0169)\n","epoch: 17 loss: tensor(1.0169)\n","epoch: 18 loss: tensor(1.0165)\n","epoch: 19 loss: tensor(1.0166)\n","epoch: 20 loss: tensor(1.0162)\n","epoch: 21 loss: tensor(1.0162)\n","epoch: 22 loss: tensor(1.0161)\n","epoch: 23 loss: tensor(1.0158)\n","epoch: 24 loss: tensor(1.0158)\n","epoch: 25 loss: tensor(1.0155)\n","epoch: 26 loss: tensor(1.0157)\n","epoch: 27 loss: tensor(1.0153)\n","epoch: 28 loss: tensor(1.0151)\n","epoch: 29 loss: tensor(1.0129)\n","epoch: 30 loss: tensor(1.0116)\n","epoch: 31 loss: tensor(1.0102)\n","epoch: 32 loss: tensor(1.0076)\n","epoch: 33 loss: tensor(1.0052)\n","epoch: 34 loss: tensor(1.0031)\n","epoch: 35 loss: tensor(1.0026)\n","epoch: 36 loss: tensor(0.9999)\n","epoch: 37 loss: tensor(0.9989)\n","epoch: 38 loss: tensor(0.9968)\n","epoch: 39 loss: tensor(0.9945)\n","epoch: 40 loss: tensor(0.9925)\n","epoch: 41 loss: tensor(0.9906)\n","epoch: 42 loss: tensor(0.9871)\n","epoch: 43 loss: tensor(0.9883)\n","epoch: 44 loss: tensor(0.9821)\n","epoch: 45 loss: tensor(0.9814)\n","epoch: 46 loss: tensor(0.9795)\n","epoch: 47 loss: tensor(0.9786)\n","epoch: 48 loss: tensor(0.9764)\n","epoch: 49 loss: tensor(0.9785)\n","epoch: 50 loss: tensor(0.9742)\n","epoch: 51 loss: tensor(0.9729)\n","epoch: 52 loss: tensor(0.9693)\n","epoch: 53 loss: tensor(0.9720)\n","epoch: 54 loss: tensor(0.9690)\n","epoch: 55 loss: tensor(0.9667)\n","epoch: 56 loss: tensor(0.9639)\n","epoch: 57 loss: tensor(0.9627)\n","epoch: 58 loss: tensor(0.9599)\n","epoch: 59 loss: tensor(0.9595)\n","epoch: 60 loss: tensor(0.9568)\n","epoch: 61 loss: tensor(0.9578)\n","epoch: 62 loss: tensor(0.9559)\n","epoch: 63 loss: tensor(0.9537)\n","epoch: 64 loss: tensor(0.9528)\n","epoch: 65 loss: tensor(0.9526)\n","epoch: 66 loss: tensor(0.9485)\n","epoch: 67 loss: tensor(0.9489)\n","epoch: 68 loss: tensor(0.9471)\n","epoch: 69 loss: tensor(0.9488)\n","epoch: 70 loss: tensor(0.9459)\n","epoch: 71 loss: tensor(0.9465)\n","epoch: 72 loss: tensor(0.9456)\n","epoch: 73 loss: tensor(0.9464)\n","epoch: 74 loss: tensor(0.9420)\n","epoch: 75 loss: tensor(0.9436)\n","epoch: 76 loss: tensor(0.9412)\n","epoch: 77 loss: tensor(0.9415)\n","epoch: 78 loss: tensor(0.9394)\n","epoch: 79 loss: tensor(0.9408)\n","epoch: 80 loss: tensor(0.9388)\n","epoch: 81 loss: tensor(0.9397)\n","epoch: 82 loss: tensor(0.9378)\n","epoch: 83 loss: tensor(0.9384)\n","epoch: 84 loss: tensor(0.9365)\n","epoch: 85 loss: tensor(0.9375)\n","epoch: 86 loss: tensor(0.9360)\n","epoch: 87 loss: tensor(0.9372)\n","epoch: 88 loss: tensor(0.9343)\n","epoch: 89 loss: tensor(0.9352)\n","epoch: 90 loss: tensor(0.9338)\n","epoch: 91 loss: tensor(0.9346)\n","epoch: 92 loss: tensor(0.9331)\n","epoch: 93 loss: tensor(0.9336)\n","epoch: 94 loss: tensor(0.9328)\n","epoch: 95 loss: tensor(0.9333)\n","epoch: 96 loss: tensor(0.9314)\n","epoch: 97 loss: tensor(0.9326)\n","epoch: 98 loss: tensor(0.9304)\n","epoch: 99 loss: tensor(0.9316)\n","epoch: 100 loss: tensor(0.9300)\n","epoch: 101 loss: tensor(0.9308)\n","epoch: 102 loss: tensor(0.9292)\n","epoch: 103 loss: tensor(0.9298)\n","epoch: 104 loss: tensor(0.9284)\n","epoch: 105 loss: tensor(0.9291)\n","epoch: 106 loss: tensor(0.9278)\n","epoch: 107 loss: tensor(0.9283)\n","epoch: 108 loss: tensor(0.9271)\n","epoch: 109 loss: tensor(0.9277)\n","epoch: 110 loss: tensor(0.9266)\n","epoch: 111 loss: tensor(0.9270)\n","epoch: 112 loss: tensor(0.9256)\n","epoch: 113 loss: tensor(0.9267)\n","epoch: 114 loss: tensor(0.9252)\n","epoch: 115 loss: tensor(0.9259)\n","epoch: 116 loss: tensor(0.9248)\n","epoch: 117 loss: tensor(0.9248)\n","epoch: 118 loss: tensor(0.9241)\n","epoch: 119 loss: tensor(0.9246)\n","epoch: 120 loss: tensor(0.9233)\n","epoch: 121 loss: tensor(0.9236)\n","epoch: 122 loss: tensor(0.9226)\n","epoch: 123 loss: tensor(0.9231)\n","epoch: 124 loss: tensor(0.9220)\n","epoch: 125 loss: tensor(0.9225)\n","epoch: 126 loss: tensor(0.9218)\n","epoch: 127 loss: tensor(0.9219)\n","epoch: 128 loss: tensor(0.9211)\n","epoch: 129 loss: tensor(0.9213)\n","epoch: 130 loss: tensor(0.9205)\n","epoch: 131 loss: tensor(0.9208)\n","epoch: 132 loss: tensor(0.9201)\n","epoch: 133 loss: tensor(0.9200)\n","epoch: 134 loss: tensor(0.9193)\n","epoch: 135 loss: tensor(0.9197)\n","epoch: 136 loss: tensor(0.9191)\n","epoch: 137 loss: tensor(0.9192)\n","epoch: 138 loss: tensor(0.9186)\n","epoch: 139 loss: tensor(0.9186)\n","epoch: 140 loss: tensor(0.9181)\n","epoch: 141 loss: tensor(0.9184)\n","epoch: 142 loss: tensor(0.9178)\n","epoch: 143 loss: tensor(0.9183)\n","epoch: 144 loss: tensor(0.9175)\n","epoch: 145 loss: tensor(0.9177)\n","epoch: 146 loss: tensor(0.9171)\n","epoch: 147 loss: tensor(0.9171)\n","epoch: 148 loss: tensor(0.9167)\n","epoch: 149 loss: tensor(0.9170)\n","epoch: 150 loss: tensor(0.9165)\n","epoch: 151 loss: tensor(0.9168)\n","epoch: 152 loss: tensor(0.9162)\n","epoch: 153 loss: tensor(0.9162)\n","epoch: 154 loss: tensor(0.9157)\n","epoch: 155 loss: tensor(0.9155)\n","epoch: 156 loss: tensor(0.9151)\n","epoch: 157 loss: tensor(0.9154)\n","epoch: 158 loss: tensor(0.9150)\n","epoch: 159 loss: tensor(0.9148)\n","epoch: 160 loss: tensor(0.9147)\n","epoch: 161 loss: tensor(0.9145)\n","epoch: 162 loss: tensor(0.9145)\n","epoch: 163 loss: tensor(0.9144)\n","epoch: 164 loss: tensor(0.9138)\n","epoch: 165 loss: tensor(0.9140)\n","epoch: 166 loss: tensor(0.9137)\n","epoch: 167 loss: tensor(0.9138)\n","epoch: 168 loss: tensor(0.9136)\n","epoch: 169 loss: tensor(0.9135)\n","epoch: 170 loss: tensor(0.9134)\n","epoch: 171 loss: tensor(0.9134)\n","epoch: 172 loss: tensor(0.9131)\n","epoch: 173 loss: tensor(0.9131)\n","epoch: 174 loss: tensor(0.9129)\n","epoch: 175 loss: tensor(0.9124)\n","epoch: 176 loss: tensor(0.9120)\n","epoch: 177 loss: tensor(0.9123)\n","epoch: 178 loss: tensor(0.9120)\n","epoch: 179 loss: tensor(0.9123)\n","epoch: 180 loss: tensor(0.9119)\n","epoch: 181 loss: tensor(0.9119)\n","epoch: 182 loss: tensor(0.9116)\n","epoch: 183 loss: tensor(0.9118)\n","epoch: 184 loss: tensor(0.9113)\n","epoch: 185 loss: tensor(0.9114)\n","epoch: 186 loss: tensor(0.9113)\n","epoch: 187 loss: tensor(0.9113)\n","epoch: 188 loss: tensor(0.9109)\n","epoch: 189 loss: tensor(0.9110)\n","epoch: 190 loss: tensor(0.9109)\n","epoch: 191 loss: tensor(0.9108)\n","epoch: 192 loss: tensor(0.9103)\n","epoch: 193 loss: tensor(0.9105)\n","epoch: 194 loss: tensor(0.9102)\n","epoch: 195 loss: tensor(0.9102)\n","epoch: 196 loss: tensor(0.9101)\n","epoch: 197 loss: tensor(0.9100)\n","epoch: 198 loss: tensor(0.9100)\n","epoch: 199 loss: tensor(0.9098)\n","epoch: 200 loss: tensor(0.9098)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BYTV81Yif0Sc"},"source":["## Testing the SAE"]},{"cell_type":"code","metadata":{"id":"v5_mJJscf3oj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727887501968,"user_tz":-240,"elapsed":396,"user":{"displayName":"Hadelin de Ponteves","userId":"15047218817161520419"}},"outputId":"5b187cc8-6463-4bd1-a662-68ee051c8c66"},"source":["test_loss = 0\n","s = 0.\n","for id_user in range(nb_users):\n","    input = Variable(training_set[id_user]).unsqueeze(0)\n","    target = Variable(test_set[id_user]).unsqueeze(0)\n","    if torch.sum(target.data > 0) > 0:\n","        output = sae(input)\n","        target.require_grad = False\n","        output[target == 0] = 0\n","        loss = criterion(output, target)\n","        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n","        test_loss += np.sqrt(loss.data*mean_corrector)\n","        s += 1.\n","print('test loss: '+str(test_loss/s))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test loss: tensor(0.9499)\n"]}]}]}